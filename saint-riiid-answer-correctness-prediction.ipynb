{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport math\nimport time\nimport gc\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dtype_train = {\n               'row_id': 'int64', \n               'timestamp': 'int64', \n               'user_id': 'int32', \n               'content_id': 'int32', \n               'content_type_id': 'int8',\n               'task_container_id': 'int16', \n               'user_answer': 'int8', \n               'answered_correctly': 'int8', \n               'prior_question_elapsed_time': 'float32', \n               'prior_question_had_explanation': 'boolean',\n                }\ndtype_questions = {\n                'question_id': 'int32',\n                'part': 'int8',\n                }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/train.csv', low_memory=False,skiprows= range(1, 9*(10**7)), \n                       dtype=dtype_train, usecols=dtype_train.keys()\n                      )\nprint (train.shape)\ntrain = train[train['content_type_id'] == 0]\nquestions = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/questions.csv', dtype=dtype_questions, \n                          usecols=dtype_questions.keys(), index_col='question_id' )\ntrain = train.join(questions, on = 'content_id')\ncols = ['content_id', 'task_container_id', 'prior_question_had_explanation', 'part']\nresp_cols = ['answered_correctly','timestamp', 'prior_question_elapsed_time']\ncols_ = cols + resp_cols + ['user_id']\nexclude_list = ['part', 'user_id']\ntrain = train[cols_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data preprocessing**\n- Add 1 to all columns to reserve 0 for embedding \n- Compute lagtime: time between a user interactions\n- Convert lagtime to minutes\n- Convert prior_question_elapsed_time to minutes and cap it to 300\n- Create a vocab_size dictionary to count the number of unique values for each column (we will need it for the embedding layers)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef preprocess_data(train, exclude_list):\n    for col in cols_:\n        if col not in exclude_list:\n            train[col] += 1\n    train.fillna({'prior_question_elapsed_time' : 0, 'prior_question_had_explanation' : 4}, inplace=True)\n    train['lagtime'] = train.groupby('user_id')['timestamp'].shift()\n    train['lagtime']=train['timestamp']-train['lagtime']\n    train['lagtime'].fillna(0, inplace=True)\n    train.lagtime=train.lagtime.astype('int32')\n    train['timestamp'] = train['lagtime']\n    train.drop('lagtime', axis='columns', inplace=True)\n    train['timestamp'] = train['timestamp'].apply(lambda x: x // 60000).astype('int32')\n    train['prior_question_elapsed_time'] = train['prior_question_elapsed_time'].apply(lambda x: min(300, x / 1000))\npreprocess_data(train,exclude_list )\nvocab_size = {}\nfor col in ['prior_question_had_explanation', 'part', 'timestamp']:\n    vocab_size[col] = int(train[col].max() + 2)\n\nvocab_size['content_id'] = 30000\nvocab_size['task_container_id'] = 20001\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Generator**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    \n    def __init__(self, train, seq_len, cols,training, start_index):\n        self.train = train\n        self.train_keys = list(self.train.keys())\n        self.cols = cols\n        self.training = training\n        self.seq_len = seq_len\n        self.start_index = start_index\n        self.user_index = dict(zip([i +  self.start_index for i in range(len(self.train))], self.train_keys))\n        np.random.shuffle(self.train_keys)\n        \n    \n    def __len__(self):\n        return len(self.train)\n    \n    \n    def rolling_window(self,a, w):\n        s0, s1 = a.strides\n        m, n = a.shape\n        return np.lib.stride_tricks.as_strided(\n            a, \n            shape=(m-w+1, w, n), \n            strides=(s0, s0, s1)\n        )\n    \n    def get_sequences(self, x, resp, y):\n        exercise = np.pad(x, [[ self.seq_len-1, 0], [0, 0]], constant_values=0)\n        resp = np.concatenate((np.array([3.0, 3.0, 3.0]).reshape(1,3), resp[:- 1,:]))\n        response = np.pad(resp, [[ self.seq_len-1, 0], [0, 0]], constant_values=0)\n        exercise = self.rolling_window(exercise, self.seq_len)\n        response = self.rolling_window(response, self.seq_len)\n        if self.training:\n            return exercise, response, y\n        else:\n                            \n            return exercise, response\n        \n            \n    \n    def __getitem__(self, index):\n        user = self.user_index[index +  self.start_index]\n        user_df = self.train[user].values\n        if len(user_df )> 150:\n            seq_ind = np.random.randint(0,len(user_df) - 150 + 1)\n            user_df = user_df[seq_ind:seq_ind + 150,:]\n        x = user_df[:,:4]\n        resp = user_df[:,4:7]\n        y = np.array(user_df[:,4])\n        if self.training:\n            exercise, response, next_y = self.get_sequences(x,resp, y)\n            return {'exercise': tf.convert_to_tensor(exercise, dtype=tf.float32),\n                    'response' : tf.convert_to_tensor(response, dtype=tf.float32)\n                   }, tf.convert_to_tensor(np.array(next_y), dtype=tf.int64)\n        else:\n            exercise, response = self.get_sequences(x,resp, y)\n            return {'exercise': tf.convert_to_tensor(exercise, dtype=tf.float32),\n                    'response' : tf.convert_to_tensor(response, dtype=tf.float32)\n                   }\n            \n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Positional Encoding**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class PositionalEncoding(tf.keras.layers.Layer):\n    def __init__(self, cols, vocab_size, d_model, max_len):\n        super(PositionalEncoding, self).__init__()\n        self.cols = cols\n        self.vocab_size = vocab_size\n        self.d_model = d_model\n        self.max_len = max_len\n        self.embeddings = []\n        for col in self.cols:\n            self.embeddings.append(tf.keras.layers.Embedding(self.vocab_size[col],self.d_model))\n    \n    def call(self, x):\n        embeddings = []\n        for i in range(len(self.cols)):\n            embeddings.append(self.embeddings[i](x[:,:,i]))\n        return tf.keras.layers.Add()(embeddings)\n        \n\ndef scaled_dot_product_attention(q, k, v, mask):\n    \"\"\"Calculate the attention weights.\n  q, k, v must have matching leading dimensions.\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n  The mask has different shapes depending on its type(padding or look ahead) \n  but it must be broadcastable for addition.\n\n  Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable \n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n  Returns:\n    output, attention_weights\n  \"\"\"\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n    # add the mask to the scaled tensor.\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)  \n\n    # softmax is normalized on the last axis (seq_len_k) so that the scores\n    # add up to 1.\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n    return output, attention_weights\n\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model // self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        \"\"\"Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n        \n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        scaled_attention, attention_weights = scaled_dot_product_attention(\n            q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n        concat_attention = tf.reshape(scaled_attention, \n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output, attention_weights\n    \ndef point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])\n\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        skip_x = x\n        x = self.layernorm1(x)\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        #attn_output = self.dropout1(attn_output, training=training)\n        #out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n        out1 = skip_x + attn_output\n        ffn_output = self.ffn(self.layernorm2(out1))\n        out2 = out1 + ffn_output\n\n        #ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        #ffn_output = self.dropout2(ffn_output, training=training)\n        #out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2\n    \nclass DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n        self.layernorm4 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n\n\n    def call(self, x, enc_output, training, \n           look_ahead_mask, padding_mask):\n        # enc_output.shape == (batch_size, input_seq_len, d_model)\n        x = self.layernorm1(x)\n        enc_output = self.layernorm4(enc_output)\n\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n        #attn1 = self.dropout1(attn1, training=training)\n        out1 = x +  attn1\n        skip_out = out1\n        out1 = self.layernorm2(out1)\n        \n\n        attn2, attn_weights_block2 = self.mha2(\n            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n        #attn2 = self.dropout2(attn2, training=training)\n        out2 = skip_out + attn2  # (batch_size, target_seq_len, d_model)\n        ffn_output = self.ffn(self.layernorm3(out2))  # (batch_size, target_seq_len, d_model)\n        #ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = out2 + ffn_output  # (batch_size, target_seq_len, d_model)\n        \n\n        return out3, attn_weights_block1, attn_weights_block2\n    \nclass Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff,\n               maximum_position_encoding, cols, vocab_dict, rate=0.1):\n        super(Encoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.cols = cols\n        self.vocab_dict = vocab_dict\n\n        self.embedding = PositionalEncoding(cols, vocab_dict, d_model, maximum_position_encoding )\n        self.pos_encoding = tf.keras.layers.Embedding(config['seq_len'], d_model)\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n                       for _ in range(num_layers)]\n\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        \n        #seq_len = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=config['seq_len'], delta=1)\n\n        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n        #x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding(positions)\n        \n        #x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n\n        return x  # (batch_size, input_seq_len, d_model)\n    \nclass Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n               maximum_position_encoding, rate=0.1):\n        super(Decoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.dense_timestamp = tf.keras.layers.Embedding(35793,d_model) \n        self.dense_elapsed = tf.keras.layers.Embedding(302,d_model)\n        #self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n        self.pos_encoding = tf.keras.layers.Embedding(config['seq_len'], d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n                       for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training, \n           look_ahead_mask, padding_mask):\n        #seq_len = tf.shape(x)[-1]\n        attention_weights = {}\n        positions = tf.range(start=0, limit=config['seq_len'], delta=1)\n        x = self.embedding(x[:,:,0])  # (batch_size, target_seq_len, d_model)\n        timestamp = self.dense_timestamp(x[:,:,1])\n        elapsed_time = self.dense_elapsed(x[:,:,2])\n        x = tf.keras.layers.Add()([x,timestamp, elapsed_time])\n        #x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        #x += self.pos_encoding[:, :seq_len, :]\n        x += self.pos_encoding(positions)\n        #x = self.dropout(x, training=training)\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                             look_ahead_mask, padding_mask)\n\n            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n\n        # x.shape == (batch_size, target_seq_len, d_model)\n        return x, attention_weights\n    \n    \nclass Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, \n               target_vocab_size, pe_input, pe_target, cols, vocab_dict, rate=0.1):\n        super(Transformer, self).__init__()\n        \n        self.cols = cols\n        self.vocab_dict = vocab_dict\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n                            pe_input,cols, vocab_dict, rate)\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n                           target_vocab_size, pe_target, rate)\n        #self.final_layer = tf.keras.layers.Dense(1, activation = 'softmax')\n        #self.output_layer = tf.keras.layers.TimeDistributed(self.final_layer)\n        #self.global_average = tf.keras.layers.GlobalAveragePooling1D()\n        #self.dropout1 = tf.keras.layers.Dropout(0.1)\n        self.final_layer = tf.keras.layers.Dense(1, activation='linear')\n        #self.dropout2 = tf.keras.layers.Dropout(0.1)\n        #self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n        #self.output_layer = tf.keras.layers.Softmax()\n\n    def call(self, inp, tar, training, enc_padding_mask, dec_padding_mask):\n\n        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n        \n\n        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n        dec_output, attention_weights = self.decoder(\n            tar, enc_output, training, enc_padding_mask, dec_padding_mask)\n        #global_average = self.global_average(dec_output)\n        #dropout1 = self.dropout1(global_average)\n        #final_output = self.final_layer(dec_output)\n        #dropout2 = self.dropout2(final_output)\n        \n\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n        #output_l = self.output_layer(dec_output)\n        \n\n        return final_output, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = {\n    'seq_len' : 70,\n    'batch_size' : 512\n}\nparams = {\n    'num_layers' : 2,\n    'd_model' : 128,\n    'num_heads' : 4,\n    'dff' : 512,\n    'target_vocab_size' : 4,\n    'pe_input' : config['seq_len'] + 2,\n    'pe_target' : config['seq_len'] + 2,\n    'cols' : cols,\n    'vocab_dict' : vocab_size\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_padding_mask(seq):\n    seq = tf.cast(tf.reduce_all(tf.math.equal(seq, 0),-1), tf.float32)\n\n    # add extra dimensions to add the padding\n    # to the attention logits.\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (seq_len, seq_len)\ndef create_masks(inp, tar):\n    # Encoder padding mask\n    enc_padding_mask = create_padding_mask(inp)\n    dec_target_padding_mask = create_padding_mask(tar)\n    # Used in the 2nd attention block in the decoder.\n    # This padding mask is used to mask the encoder outputs.\n    #dec_padding_mask = create_padding_mask(inp)\n\n    # Used in the 1st attention block in the decoder.\n    # It is used to pad and mask future tokens in the input received by \n    # the decoder.\n    look_ahead_mask = create_look_ahead_mask(config['seq_len'])\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n    combined_encoder_mask = tf.maximum(enc_padding_mask, look_ahead_mask)\n    return combined_encoder_mask, combined_mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=500):\n        super(CustomSchedule, self).__init__()\n\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)/100\nlearning_rate = CustomSchedule(params['d_model'])\noptimizer = tf.keras.optimizers.Adam(learning_rate =learning_rate, beta_1=0.9, beta_2=0.999, \n                                     epsilon=1e-8)\n#optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4)\nloss_object = tf.keras.losses.binary_crossentropy\nauc = tf.keras.metrics.AUC( name='AUC')\nbin_acc = tf.keras.metrics.BinaryAccuracy(name='acc')\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\ntrain_auc = tf.keras.metrics.Mean(name='train_auc')\nval_auc = tf.keras.metrics.AUC( name='val_AUC')\nval_bin_acc = tf.keras.metrics.BinaryAccuracy(name='val_acc')\nval_loss = tf.keras.metrics.Mean(name='val_loss')\nvalidation_accuracy = tf.keras.metrics.Mean(name='val_accuracy')\nvalidation_auc = tf.keras.metrics.Mean(name='val_auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot  as plt\ntemp_learning_rate_schedule = CustomSchedule(params['d_model'])\nplt.plot(temp_learning_rate_schedule(tf.range(3500, dtype=tf.float32)))\nplt.ylabel(\"Learning Rate\")\nplt.xlabel(\"Train Step\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_function(real, pred):\n    real = real - 1\n    real = real[:, np.newaxis]\n    loss_ = loss_object(real, pred)\n\n\ndef accuracy_function(y_true, y_pred, bin_acc):\n    y_true = y_true - 1\n    y_true = y_true[:, np.newaxis]\n    bin_acc.update_state(y_true, y_pred)\n    acc = bin_acc.result()\n    return tf.math.reduce_mean(acc)\ndef auc_function(y_true, y_pred, auc):\n    \n    y_true = y_true - 1\n    y_true = y_true[:, np.newaxis]\n    auc.update_state(y_true, y_pred)\n    auc_ = auc.result()\n    return tf.math.reduce_mean(auc_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_step_signature = [\n    {'exercise': tf.TensorSpec(shape=(None,None,None), dtype=tf.float32),\n      'response':tf.TensorSpec(shape=(None,None,None), dtype=tf.float32)},\n    tf.TensorSpec(shape=(None,  ), dtype=tf.int64),\n                           ]\n\ntransformer = Transformer(**params)\ntransformer.load_weights('../input/checkpoints4/my_checkpoint')\n    \n@tf.function(input_signature=train_step_signature)\ndef train_step(x, y):\n    exercise = x['exercise']\n    response = x['response']\n    enc_padding_mask, dec_padding_mask = create_masks(exercise, response)\n    with tf.GradientTape() as tape:\n        predictions, _ = transformer(exercise, response, \n                                True, \n                                enc_padding_mask, \n                                dec_padding_mask)\n        y = y - 1\n        y = y[:, np.newaxis]\n        predictions = tf.math.sigmoid(predictions)\n        predictions = predictions[:,-1,:]\n        loss = loss_object(y , predictions)\n    gradients = tape.gradient(loss, transformer.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n    bin_acc.update_state(y , predictions)\n    auc.update_state(y, predictions)\n    train_loss.update_state(loss)\n   \n        \n@tf.function(input_signature=train_step_signature)\ndef test_step(x, y):\n    exercise = x['exercise']\n    response = x['response']\n    enc_padding_mask, dec_padding_mask = create_masks(exercise, response)\n    predictions, _ = transformer(exercise, response, \n                                False, \n                                enc_padding_mask, \n                                dec_padding_mask)\n    predictions =tf.math.sigmoid(predictions)\n    predictions = predictions[:,-1,:]\n    y = y - 1\n    y = y[:, np.newaxis]\n    loss = loss_object(y , predictions)\n    val_bin_acc.update_state(y , predictions)\n    val_auc.update_state(y, predictions)\n    val_loss.update_state(loss)\n    \n\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_idx = int(len(train)*0.8)\ntrain_df = train[:train_idx]\ntrain_group = dict(tuple(train_df.groupby('user_id')))\ndel train_df\ngc.collect()\nval_df = train[train_idx:]\nval_group = dict(tuple(val_df.groupby('user_id')))\ndel val_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef update_train_states():\n    train_loss.reset_states()\n    bin_acc.reset_states()\n    auc.reset_states()\ndef update_val_states():\n    val_loss.reset_states()\n    val_bin_acc.reset_states()\n    val_auc.reset_states()\n    \nfor epoch in range(3):\n    print (f'****************************Epoch {epoch}************************************** ')\n    train_iter = DataGenerator2(train_group,config['seq_len'], cols, True,0)\n    steps = int(len(train_iter) )\n    print('training steps', steps)\n    count = 0\n    countb = 0\n    exercises = []\n    responses = []\n    ys = []\n    for batch, (x,y) in enumerate(train_iter):\n        exercises.extend(x['exercise'])\n        responses.extend(x['response'])\n        ys.extend(y)\n        countb += y.get_shape().as_list()[0]\n        if countb >= config['batch_size']:\n            (x_,y_) = {'exercise': tf.convert_to_tensor(exercises, dtype=tf.float32),\n                    'response' : tf.convert_to_tensor(responses, dtype=tf.float32)\n                    }, tf.convert_to_tensor(ys, dtype=tf.int64)\n            countb = 0\n            exercises = []\n            responses = []\n            ys = []\n            train_step(x_,y_)\n            if count % 100 ==0:\n                print ('Batch {} Loss {:.4f} Accuracy {:.4f} AUC {:.4f}'.format(\n                     batch, train_loss.result(), bin_acc.result(), auc.result()))\n            count += 1\n            if count > steps:\n                break\n   \n    update_train_states()\n    print ('validation')\n    val_iter = DataGenerator2(val_group,config['seq_len'], cols, True,train_idx)\n    val_steps =int(len(val_iter) )\n    print('validation steps', val_steps)\n    auc_score = []\n    valcount = 0\n    countb = 0\n    exercises = []\n    responses = []\n    ys = []\n    for batch, (x,y) in enumerate(val_iter): \n        exercises.extend(x['exercise'])\n        responses.extend(x['response'])\n        ys.extend(y)\n        countb += y.get_shape().as_list()[0]\n        if countb >= config['batch_size'] or countb >= val_steps:\n            (x_,y_) = {'exercise': tf.convert_to_tensor(exercises, dtype=tf.float32),\n                    'response' : tf.convert_to_tensor(responses, dtype=tf.float32)\n                    }, tf.convert_to_tensor(ys, dtype=tf.int64)\n            countb = 0\n            exercises = []\n            responses = []\n            ys = []\n            test_step(x_, y_)\n            auc_score.append(val_auc.result())\n        if valcount % 100 ==0:\n            print ('Batch {} Loss {:.4f} Accuracy {:.4f} AUC {:.4f}'.format(\n                 batch, val_loss.result(), val_bin_acc.result(), val_auc.result()))\n        valcount += 1\n        if valcount > val_steps:\n            break\n    print ('validation score', np.array(auc_score).mean())\n    update_val_states()\n        \n        \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer.save_weights('./checkpoints/my_checkpoint')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_group = train.groupby('user_id')\ntrain_dict = dict(tuple(train_group))\ncolumns = cols + ['timestamp', 'prior_question_elapsed_time']+ ['user_id', 'row_id']\ndel train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_train(prior_test):\n    #prior_test['answered_correctly'] = np.array(prior_correct)\n    #train_df = pd.concat([train_df, prior_test])\n    test_group = dict(tuple(prior_test.groupby('user_id')))\n    for user in test_group.keys():\n        if user in train_dict.keys():\n            train_dict[user] = pd.concat([train_dict[user],test_group[user]]).tail(config['seq_len'])\n        else:\n            train_dict[user] = test_group[user].tail(config['seq_len'])    \n            \n    #user_dict.update(test_group.size().to_dict())\n    #train_dict = train_group.size().to_dict()\n    #return train_df, train_group\ndef preprocess_test(train, exclude_list):\n    for col in cols:\n        if col not in exclude_list:\n            train[col] += 1\n    train.fillna({'prior_question_elapsed_time' : 0, 'prior_question_had_explanation' : 4}, inplace=True)\n    train['lagtime'] = train.groupby('user_id')['timestamp'].shift()\n    train['lagtime']=train['timestamp']-train['lagtime']\n    train['lagtime'].fillna(0, inplace=True)\n    train.lagtime=train.lagtime.astype('int32')\n    train['timestamp'] = train['lagtime']\n    train.drop('lagtime', axis='columns', inplace=True)\n    # convert timestamp to number of days and cap the number to 600\n    #train['timestamp'] = train['timestamp'].apply(lambda x: min(600, max(x / 86400000,0)))\n    train['timestamp'] = train['timestamp'].apply(lambda x: min(x // 60000, 35790)).astype('int32')\n    #max_time_stamp = max(train['timestamp'].values.max(),1)\n    #train['timestamp'] /=max_time_stamp\n    #convert prior_question_elapsed_time to seconds and cap the max to 300\n    train['prior_question_elapsed_time'] = train['prior_question_elapsed_time'].apply(lambda x: min(300, x / 1000))\n    \n    \ndef rolling_window(a, w):\n        s0, s1 = a.strides\n        m, n = a.shape\n        return np.lib.stride_tricks.as_strided(\n            a, \n            shape=(m-w+1, w, n), \n            strides=(s0, s0, s1)\n        )\n\ndef generate_sequence(row, seq_len = config['seq_len']):\n    #user  = row['user_id']\n    user = row[-1]\n    if user  in train_dict.keys():\n        data = train_dict[user].tail(seq_len - 1).values\n        pad_size =  seq_len - len(data) - 1\n        x = np.pad(np.concatenate([data[:,:4],row[:4].reshape(1,4)]), [[ pad_size , 0], [0, 0]], constant_values=0)\n        x = rolling_window(x, seq_len) \n        y = np.concatenate((np.array([3.0, 3.0, 3.0]).reshape(1,3), data[:,4:7]))\n        y = np.pad(y, [[ pad_size , 0], [0, 0]], constant_values=0)     \n        y = rolling_window(y, seq_len) \n    else:\n        x = np.pad(row[:4].reshape(1,4), [[ seq_len - 1 , 0], [0, 0]], constant_values=0)\n        x = rolling_window(x, seq_len)\n        y = np.concatenate([[0,0,0] * (seq_len - 1),[3.,3.,3.]]).reshape(seq_len,3)\n        y = rolling_window(y, seq_len) \n    return {'exercise':x, 'response':y }  \n    #return item","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_step_signature = {'exercise': tf.TensorSpec(shape=(None,None,None), dtype=tf.float32),\n      'response':tf.TensorSpec(shape=(None,None,None), dtype=tf.float32)}\n                           \n@tf.function(input_signature=[predict_step_signature])\n#@tf.function\ndef predict_step(x):\n    exercise = x['exercise']\n    response = x['response']\n    enc_padding_mask, dec_padding_mask = create_masks(exercise, response)\n    predictions, _ = transformer(exercise, response, \n                                False, \n                                enc_padding_mask, \n                                dec_padding_mask)\n    predictions =tf.math.sigmoid(predictions)\n    predictions = predictions[:,-1,:]\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import riiideducation\nenv = riiideducation.make_env()\niter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for test, sample_prediction in iter_test:\n   \n    \n    try:\n        prior_correct = eval(test['prior_group_answers_correct'].iloc[0])\n        prior_correct = [a +1   for a in prior_correct if a != -1]\n    except:\n        prior_correct = []\n    \n    # Add prior correct to test and update stored users\n    if prior_correct:\n        prior_test.insert(4,'answered_correctly', prior_correct)\n        #prior_test['answered_correctly'] = prior_correct\n        update_train(prior_test)\n        \n\n    # Filter test\n    test = test.loc[\n        test['content_type_id'] == 0]\n    # add questions\n    test = test.join(questions, on = 'content_id')\n    test = test[columns]\n\n    # Add global features\n    preprocess_test(test, exclude_list)\n\n    # Save test for later\n    prior_test = test.drop(columns='row_id').copy()\n\n    # Make x\n    sequences = np.apply_along_axis(\n        generate_sequence,\n        1,\n        test.drop(columns='row_id').to_numpy()\n    )\n    x = [np.squeeze(s['exercise'], axis = 0) for s in sequences]\n    y = [np.squeeze(s['response'], axis = 0) for s in sequences]\n    items = {'exercise': tf.convert_to_tensor(x, dtype=tf.float32),\n                'response' : tf.convert_to_tensor(y, dtype=tf.float32)\n                }\n     # Predict\n    predictions = predict_step(items)\n    test['answered_correctly'] = predictions.numpy()\n\n    \n    env.predict(test[['row_id', 'answered_correctly']])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}